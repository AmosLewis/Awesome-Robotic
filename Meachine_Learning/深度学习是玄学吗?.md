##  如何看待深度学习是玄学的论调？


最近在整理以前学习过的机器学习，深度学习，视觉定位和导航， 计算机视觉几个学科的时候，发现了一些有趣的联系。 逛知乎的时候偶然发现了一个论述，说
机器学习是统计，而深度学习已经脱离了统计，陷入了玄学的范畴。 这个好像也是周围很多人的观点。 我自己思考了一下这个问题，下文是我的思考。

机器学习的有参数模型（暂时不讨论无参数模型），本质上还是用统计方法进行参数估计。深度学习用神经网络模拟超级复杂的函数，普通的机器学习比如线性回归模型是在给定普通函数然后去估计期参数，
假定普通函数的结果服从某种分布，而这种分布对应的模型我们成为广义非线性模型（GLM），回归中这种GLM对应分布就是指数族分布，比如线性回归中的高斯分布对应的分布函数，逻辑回归中的二项分布对应的分布函数 p = 1/(1+e^ax),都是指数族分布的特例。觉得玄是因为要模拟的函数超级复杂，参数的意义不明确，但其实本质上
和y=ax的a是什么意义，我们也是不好讲的，我们觉得y=ax好理解，是因为习以为常，见怪不怪，而不是因为我们真的理解它，这就要上升到哲学高度去讨论了。

另一方面，深度学习所解决的目标，他的概率分布一般也是无法显示给出，其实本质上是用到一种我们称为递归广义非线性模型，即分层的非线性递归函数去模拟种种分布，现实世界中复杂问题的复杂，就复杂在其正确答案的分布是无法用简单的概率分布去模拟的，
而深度学习通过计算一种复杂参数去模拟了这种真实世界的复杂概率分布,具体如何模拟去看下文给的链接。如果非要讲理解这样的玄学，那请你先理解人脑处理复杂问题的逻辑。事实上我们暂时是无法理解的，
其实我们也不需要理解，我们处理现实问题就是一种见多了就知道怎么去处理问题的，这就是所谓的社会经验，这就是个训练的过程，这个训练的过程就是一个统计的过程，
而不是一个因果关系推导的过程。

当我们妄图通过简单的因果律去理解世界时，我们会发现很多事情不可理解，这就是个哲学问题，这个世界本质是不可理解的，只是我们看多了觉得某件事情就应该是这样，
这就产生了因果律。就像在《西方美学》一书中，詹姆逊分析了阿尔杜塞为代表的结构主义马克思主义对传统马克思主义的批判，批判传统的马克思主义的缺陷在与其
机械因果律和表现性因果律。分析中说，机械因果律在文化分析中仍然存在局部合法性，因为他仍然是我们这个特别堕落的现实社会的现实法则之一，与此同时，
表现性因果律也是我们的历史现实内部的局部法则之一。社会科学领域的得到的结论与我们数学领域得到的结论多么的相似。我们总是会陷入某种局部极值，
但很多问题中这种局部极值就是堪用的，这就是为什么机器学习中为什么总是能够给一些很强的假设，去推导(因果链)出一个结论，这个结论本质上由于假设不成立，
就应该是错的，但事实告诉我们这个结论是看用的。但为什么这个错误的结论却是看用的呢？这个问题在往后问你也是哲学问题进而变成玄学吗。
我们都知道我们不能总是去问为什么，因为迟早有一个为什么所有人都答不出来，我们看似严丝合缝的因果推理都建立在一些不能讲为什么的公理上的。
如果这就是玄学，那么所有的数学公理都是玄学。但在现实生活中我们不去思考为什么，思考为什么是在帮助我们解决怎么干的问题。对于普通人而言，
大部分情况下思考本身不是目的，而是手段。所以当我们去思考深度学习本身的时候，我们不是为了思考他而思考他，而是为了用它解决局部问题。
当我们把所有用因果律思考不了的问题都乘之为玄学，本省就是个逻辑怪圈。因为所有的问题都是基于一些玄学的不可理解的假设而推导出来的，
但所有问题是不是都可以称之为玄学问题。有人说现代科学的基础是可以证伪，这来源于波普尔对科学实在性的定义，这个定义本身在科学哲学界也饱受批判。
我们姑且假设这个标准成立，是真理(其实不是)。那么如何去理解数学的不可真伪即不科学性？其实这个问题问错了，这是一个数学实在论的问题。
因为我们对科学的“科学性”的判断是往往是基于a是否基于经验的或者实证的证据b是否具有定量特征。 所以问数学是否科学（而不是玄学)，就好像是在问，
哺乳动物是猴子吗？问题就问错了。

我们再回过头来看数学领域具体问题。复杂问题如果可以通过简单因果逻辑理解，那就不是复杂问题，而是简单问题，可以逻辑推导（因果律）。
而深度学习的起因是模拟人脑的神经元去做复杂问题的求解，所以具体化逻辑上看不懂才是深度学习固有属性，如果我们看懂了，其实就已经不需要深度学习了。
但跳出具像化的逻辑，从抽象化的角度来看，这件事本质还是个用统计方法去做参数估计的过程，这里的参数就是神经元之间的连接线，激活函数帮助我们把连接线之间
的线性关系转换成非线性关系，所以我们才能模拟非线性函数进而模拟这个世界，对于具体问题用什么非线性函数呢，就是涉及到参数化的问题。而参数的更新用梯度下降，是因为复杂函数，无法使用解析解。何为解析解？在线性回归中，我们理论上是可以公式推理，
用线性代数，通过SVD分解去算样本的伪逆而解出超定方程的结果，但当样本过大时，SVD时矩阵逆运算时间复杂度是O(n^3)的，计算量超级大，我们想要绕过这个时间复杂度，
所以我们选用了梯度下降去更新参数，，当参数更新值不大时，我们称之为收敛，就求出了参数（参数估计)，也即求出了从输入x到输出y的映射，这个过程是个定量化求解
的过程，也是基于实证的，符合我们上文对科学的理解。而线性回归的函数恰好是一个凸函数，局部极恰好值就是全局最值(运气好)，这还是一种可以理解的简单函数，
而深度学习或者深度学习有时候work有时候不work, 也是因为梯度下降本身不能求全局极值，或者就是因为样本不够，无法覆盖全局函数，也就无法估计全局函数的参数，
就好比你让一个还没有看过社会复杂人心险恶的儿童，去面对处理成人社会的复杂问题，本来就是不可行的教育逻辑(训练逻辑)。所以我们说机器学习为什么叫学习，
为什么叫机器，这个机器的本质就是对于客观世界参数化得来的参数，学习的本质就是统计，机器学习的本质就是用统计方法去做参数估计。
而深度学习是机器学习的子集所以还是统计。

所以当有人说深度学习脱离统计，陷入玄学，一是统计的理解还不够深入，二是对于统计学 数学 科学之间的关系没搞明白。

[从统计学角度来看深度学习](https://cosx.org/2015/05/a-statistical-view-of-deep-learning-i-recursive-glms/)
